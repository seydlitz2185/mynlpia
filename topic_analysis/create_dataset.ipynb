{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.zh.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('smalldata_washed.csv')\n",
    "rumor = data['rumor'].to_list()\n",
    "reverse = data['reverse'].to_list()\n",
    "rumor_class = len(rumor)*[1]\n",
    "reverse_class = len(reverse)*[0]\n",
    "data = rumor + reverse\n",
    "data_class = rumor_class + reverse_class\n",
    "#后续添加数据要放在raw目录下，使用.xlsx格式将后续标注的数据加入data\n",
    "raw_data_list = os.listdir('raw')\n",
    "data_list = []\n",
    "good_name = re.compile(r'^(?!(\\~\\$)).*(.xlsx)')\n",
    "for i in raw_data_list:\n",
    "    if good_name.match(i):\n",
    "    #    data_list.append(i)\n",
    "        temp = pd.read_excel('raw/'+i)\n",
    "        temp.fillna('',inplace=True)\n",
    "        temp_rumor = [x.strip() for x in  temp['punc_rumor'].to_list()  if x.strip()!='']\n",
    "        temp_reverse = [x.strip() for x in  temp['punc_truth'].to_list()  if x.strip()!='']\n",
    "        temp_rumor_class = len(temp_rumor)*[1]\n",
    "        temp_reverse_class = len(temp_reverse)*[0]\n",
    "        temp_data = temp_rumor + temp_reverse\n",
    "        temp_data_class = temp_rumor_class + temp_reverse_class\n",
    "        for i in range( len(temp_data)):\n",
    "            temp_data[i]=re.sub(r'[\\,\\'\\ ]|(\\n)','',temp_data[i])\n",
    "    data = data + temp_data\n",
    "    data_class = data_class + temp_data_class\n",
    "with open('data.txt','w') as f:\n",
    "    for i in range(len(data)):\n",
    "        f.write(data[i]+'\\t'+str(data_class[i])+'\\n')\n",
    "        data_list =list( zip(data, data_class))\n",
    "random.shuffle(data_list)\n",
    "data, data_class = zip(*data_list)\n",
    "index = ['rumor{}{}'.format(i, '!' * j) for (i, j) in zip(range(len(data)), data_class)]\n",
    "df = pd .DataFrame(data=zip(data_class,data), columns=[\"rumor\",\"text\"], index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba分词\n",
    "import jieba\n",
    "# 去除停用词\n",
    "with open ('hit_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    " \n",
    "seg_list = []\n",
    "for i in range(len(df)):\n",
    "    words = jieba.cut(df[\"text\"].iloc[i],cut_all=False)\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    seg_list.append( ' '.join(words))\n",
    " \n",
    "df['seg_text'] = seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenyu/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize,ngram_range=(1,2))\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=df.seg_text).toarray()#这句话计算了tfidf\n",
    "words = tfidf_model.get_feature_names_out()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarities = cosine_similarity(tfidf_docs)\n",
    "cos_avg = sum([sum(row) for row in cosine_similarities])/len(cosine_similarities)\n",
    "cos_sim = [sum(row)/cos_avg for row in cosine_similarities]\n",
    "df['cos_sim'] = cos_sim\n",
    "def get_word_vector(word):\n",
    "    return ft.get_word_vector(word)\n",
    "for i,word in enumerate(words):\n",
    "    if i == 0:\n",
    "        word_vectors = get_word_vector(word)\n",
    "    else:\n",
    "        word_vectors = np.vstack((word_vectors,get_word_vector(word)))\n",
    "word_dict = dict(zip(words,word_vectors))\n",
    "np.savez('../dataset/word_dict.npz',word_dict = word_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snowNLP情感分析\n",
    "from snownlp import SnowNLP\n",
    "sentimentslist = []\n",
    "for i in (df['text']):\n",
    "    a1 = SnowNLP(i)\n",
    "    a2 = a1.sentiments\n",
    "    sentimentslist.append(a2)\n",
    "avg_len = sum([len(x) for x in df['text']])/len(df['text'])\n",
    "df['length'] = [len(x)/avg_len for x in df['text']]\n",
    "df['sentiments'] = sentimentslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rumor0</th>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor1</th>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor2</th>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor3</th>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor4</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor5</th>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "rumor0  -0.025   0.011   0.019  -0.079  -0.085  -0.020  -0.034   0.076   \n",
       "rumor1  -0.013  -0.039  -0.016   0.000   0.092  -0.057  -0.032  -0.020   \n",
       "rumor2   0.033  -0.009  -0.028   0.020   0.061   0.085   0.010   0.070   \n",
       "rumor3  -0.017   0.011  -0.042   0.003  -0.026   0.012  -0.019  -0.033   \n",
       "rumor4  -0.012   0.019   0.032  -0.057  -0.043  -0.022   0.020  -0.047   \n",
       "rumor5  -0.019   0.041  -0.021   0.034   0.012  -0.016  -0.042  -0.037   \n",
       "\n",
       "        topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "rumor0   0.020  -0.009  ...   -0.022    0.029   -0.011    0.022   -0.059   \n",
       "rumor1   0.038  -0.023  ...   -0.017   -0.030   -0.001    0.016    0.003   \n",
       "rumor2  -0.114  -0.040  ...    0.027   -0.013    0.051    0.056   -0.007   \n",
       "rumor3  -0.012   0.011  ...   -0.003    0.020    0.006   -0.025   -0.031   \n",
       "rumor4  -0.024  -0.016  ...   -0.021   -0.004   -0.027    0.011    0.034   \n",
       "rumor5  -0.021  -0.024  ...    0.030    0.050   -0.000   -0.022    0.021   \n",
       "\n",
       "        topic95  topic96  topic97  topic98  topic99  \n",
       "rumor0    0.058   -0.022   -0.008    0.012    0.004  \n",
       "rumor1   -0.036   -0.033   -0.024   -0.023   -0.033  \n",
       "rumor2   -0.096   -0.031   -0.017   -0.075    0.038  \n",
       "rumor3   -0.012   -0.018    0.068   -0.003   -0.004  \n",
       "rumor4   -0.003   -0.017   -0.004    0.001   -0.007  \n",
       "rumor5    0.010    0.011    0.009   -0.009   -0.017  \n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "k=min(len(tfidf_model.vocabulary_),len(df))\n",
    "k=100\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns, index=index)\n",
    "pca_topic_vectors.round(3).head(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca_topic_vectors'] = pca_topic_vectors.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenyu/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rumor0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "rumor0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "rumor1    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "rumor2    0.01    0.01    0.01    0.01    0.01    0.01    0.01    0.01   \n",
       "rumor3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "rumor4    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "\n",
       "        topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "rumor0    0.00    0.00  ...     0.00     0.00     0.00     0.00     0.00   \n",
       "rumor1    0.00    0.00  ...     0.00     0.00     0.92     0.00     0.00   \n",
       "rumor2    0.01    0.01  ...     0.01     0.01     0.01     0.01     0.01   \n",
       "rumor3    0.00    0.00  ...     0.00     0.00     0.00     0.00     0.00   \n",
       "rumor4    0.00    0.00  ...     0.00     0.00     0.00     0.79     0.00   \n",
       "\n",
       "        topic95  topic96  topic97  topic98  topic99  \n",
       "rumor0     0.00     0.00     0.00     0.00     0.00  \n",
       "rumor1     0.00     0.00     0.00     0.00     0.00  \n",
       "rumor2     0.01     0.01     0.01     0.01     0.01  \n",
       "rumor3     0.00     0.00     0.00     0.00     0.00  \n",
       "rumor4     0.00     0.00     0.00     0.00     0.00  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=df.text)\n",
    "                        .toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\n",
    "                                     counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia = LDiA(n_components=k, learning_method='batch')\n",
    "ldia = ldia.fit(bow_docs)  # <1>\n",
    "ldia.components_.shape\n",
    "ldia32_topic_vectors = ldia.transform(bow_docs)\n",
    "columns32 = ['topic{}'.format(i) for i in range(ldia.components_.shape[0])]\n",
    "ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors,index=index, columns=columns32)\n",
    "ldia32_topic_vectors.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ldia_topic_vectors'] = ldia32_topic_vectors.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../dataset/data.csv',encoding='utf-8')\n",
    "data = pd.read_csv('../dataset/data.csv',encoding='utf-8',index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
