{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('smalldata_washed.csv')\n",
    "rumor = data['rumor'].to_list()\n",
    "reverse = data['reverse'].to_list()\n",
    "rumor_class = len(rumor)*[1]\n",
    "reverse_class = len(reverse)*[0]\n",
    "data = rumor + reverse\n",
    "data_class = rumor_class + reverse_class\n",
    "#后续添加数据要放在raw目录下，使用.xlsx格式将后续标注的数据加入data\n",
    "raw_data_list = os.listdir('raw')\n",
    "data_list = []\n",
    "good_name = re.compile(r'^(?!(\\~\\$)).*(.xlsx)')\n",
    "for i in raw_data_list:\n",
    "    if good_name.match(i):\n",
    "    #    data_list.append(i)\n",
    "        temp = pd.read_excel('raw/'+i)\n",
    "        temp.fillna('',inplace=True)\n",
    "        temp_rumor = [x.strip() for x in  temp['punc_rumor'].to_list()  if x.strip()!='']\n",
    "        temp_reverse = [x.strip() for x in  temp['punc_truth'].to_list()  if x.strip()!='']\n",
    "        temp_rumor_class = len(temp_rumor)*[1]\n",
    "        temp_reverse_class = len(temp_reverse)*[0]\n",
    "        temp_data = temp_rumor + temp_reverse\n",
    "        temp_data_class = temp_rumor_class + temp_reverse_class\n",
    "        for i in range( len(temp_data)):\n",
    "            temp_data[i]=re.sub(r'[\\,\\'\\ ]|(\\n)','',temp_data[i])\n",
    "    data = data + temp_data\n",
    "    data_class = data_class + temp_data_class\n",
    "with open('data.txt','w') as f:\n",
    "    for i in range(len(data)):\n",
    "        f.write(data[i]+'\\t'+str(data_class[i])+'\\n')\n",
    "        data_list =list( zip(data, data_class))\n",
    "random.shuffle(data_list)\n",
    "data, data_class = zip(*data_list)\n",
    "index = ['rumor{}{}'.format(i, '!' * j) for (i, j) in zip(range(len(data)), data_class)]\n",
    "df = pd .DataFrame(data=zip(data_class,data), columns=[\"rumor\",\"text\"], index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenyu/opt/anaconda3/envs/nlpiaenv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# jieba分词\n",
    "import jieba\n",
    "# 去除停用词\n",
    "with open ('hit_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    " \n",
    "seg_list = []\n",
    "for i in range(len(df)):\n",
    "    words = jieba.cut(df[\"text\"].iloc[i],cut_all=False)\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    seg_list.append( ' '.join(words))\n",
    " \n",
    "df['seg_text'] = seg_list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize,ngram_range=(1,2))\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=df.seg_text).toarray()#这句话计算了tfidf\n",
    "tfidf_docs.shape\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_matrix = cosine_similarity(tfidf_docs)\n",
    "df['cos_matrix'] = cos_matrix.tolist()\n",
    "#snowNLP情感分析\n",
    "from snownlp import SnowNLP\n",
    "sentimentslist = []\n",
    "for i in (df['text']):\n",
    "    a1 = SnowNLP(i)\n",
    "    a2 = a1.sentiments\n",
    "    sentimentslist.append(a2)\n",
    "df['length'] = [len(x) for x in df['text']]\n",
    "df['sentiments'] = sentimentslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rumor0</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor2</th>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor3</th>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor4</th>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor5!</th>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "rumor0   -0.006   0.042   0.016   0.001  -0.030   0.007  -0.024  -0.014   \n",
       "rumor1    0.000  -0.080  -0.005   0.004  -0.083   0.028  -0.011   0.019   \n",
       "rumor2   -0.020   0.003  -0.015  -0.038   0.005   0.015  -0.002  -0.010   \n",
       "rumor3   -0.021   0.033   0.020  -0.045  -0.028   0.020  -0.043  -0.040   \n",
       "rumor4   -0.020   0.020  -0.034  -0.013  -0.015   0.008  -0.007   0.006   \n",
       "rumor5!  -0.017   0.024  -0.031  -0.023   0.002   0.012  -0.010  -0.023   \n",
       "\n",
       "         topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "rumor0    0.010   0.001  ...   -0.005   -0.009   -0.040    0.015    0.027   \n",
       "rumor1   -0.004  -0.063  ...   -0.167    0.068   -0.096    0.056   -0.056   \n",
       "rumor2   -0.004   0.034  ...   -0.044    0.003    0.031    0.006   -0.026   \n",
       "rumor3   -0.008   0.031  ...    0.129    0.001   -0.037   -0.008    0.055   \n",
       "rumor4   -0.007   0.020  ...   -0.002   -0.008   -0.010   -0.005    0.004   \n",
       "rumor5!  -0.006   0.025  ...    0.002   -0.008   -0.011    0.002    0.007   \n",
       "\n",
       "         topic95  topic96  topic97  topic98  topic99  \n",
       "rumor0    -0.022   -0.014    0.001    0.008   -0.041  \n",
       "rumor1     0.024   -0.040   -0.027   -0.144    0.085  \n",
       "rumor2     0.027    0.026   -0.013   -0.012   -0.012  \n",
       "rumor3    -0.030   -0.030   -0.017    0.075   -0.008  \n",
       "rumor4    -0.007    0.002    0.017   -0.002   -0.006  \n",
       "rumor5!   -0.007    0.003   -0.003   -0.004   -0.011  \n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "k=min(len(tfidf_model.vocabulary_),len(df))\n",
    "k=100\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns, index=index)\n",
    "pca_topic_vectors.round(3).head(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca_topic_vectors'] = pca_topic_vectors.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenyu/opt/anaconda3/envs/nlpiaenv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic22</th>\n",
       "      <th>topic23</th>\n",
       "      <th>topic24</th>\n",
       "      <th>topic25</th>\n",
       "      <th>topic26</th>\n",
       "      <th>topic27</th>\n",
       "      <th>topic28</th>\n",
       "      <th>topic29</th>\n",
       "      <th>topic30</th>\n",
       "      <th>topic31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rumor0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "rumor0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "rumor1    0.02    0.02    0.02    0.02    0.02    0.02    0.02    0.02   \n",
       "rumor2    0.01    0.01    0.84    0.01    0.01    0.01    0.01    0.01   \n",
       "rumor3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "rumor4    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "\n",
       "        topic8  topic9  ...  topic22  topic23  topic24  topic25  topic26  \\\n",
       "rumor0    0.00    0.00  ...     0.00     0.00     0.00     0.00     0.00   \n",
       "rumor1    0.02    0.02  ...     0.02     0.02     0.02     0.02     0.02   \n",
       "rumor2    0.01    0.01  ...     0.01     0.01     0.01     0.01     0.01   \n",
       "rumor3    0.00    0.00  ...     0.00     0.00     0.00     0.00     0.00   \n",
       "rumor4    0.00    0.00  ...     0.00     0.00     0.21     0.00     0.00   \n",
       "\n",
       "        topic27  topic28  topic29  topic30  topic31  \n",
       "rumor0     0.00     0.00     0.00     0.00     0.00  \n",
       "rumor1     0.02     0.52     0.02     0.02     0.02  \n",
       "rumor2     0.01     0.01     0.01     0.01     0.01  \n",
       "rumor3     0.00     0.00     0.00     0.00     0.00  \n",
       "rumor4     0.00     0.00     0.00     0.00     0.00  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=df.text)\n",
    "                        .toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\n",
    "                                     counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia = LDiA(n_components=32, learning_method='batch')\n",
    "ldia = ldia.fit(bow_docs)  # <1>\n",
    "ldia.components_.shape\n",
    "ldia32_topic_vectors = ldia.transform(bow_docs)\n",
    "columns32 = ['topic{}'.format(i) for i in range(ldia.components_.shape[0])]\n",
    "ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors,index=index, columns=columns32)\n",
    "ldia32_topic_vectors.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ldia_topic_vectors'] = ldia32_topic_vectors.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../dataset/data.csv',encoding='utf-8')\n",
    "data = pd.read_csv('../dataset/data.csv',encoding='utf-8',index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
