{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.zh.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('smalldata_washed.csv')\n",
    "rumor = data['rumor'].to_list()\n",
    "reverse = data['reverse'].to_list()\n",
    "rumor_class = len(rumor)*[1]\n",
    "reverse_class = len(reverse)*[0]\n",
    "data = rumor + reverse\n",
    "data_class = rumor_class + reverse_class\n",
    "#后续添加数据要放在raw目录下，使用.xlsx格式将后续标注的数据加入data\n",
    "raw_data_list = os.listdir('raw')\n",
    "data_list = []\n",
    "good_name = re.compile(r'^(?!(\\~\\$)).*(.xlsx)')\n",
    "for i in raw_data_list:\n",
    "    if good_name.match(i):\n",
    "    #    data_list.append(i)\n",
    "        temp = pd.read_excel('raw/'+i)\n",
    "        temp.fillna('',inplace=True)\n",
    "        temp_rumor = [x.strip() for x in  temp['punc_rumor'].to_list()  if x.strip()!='']\n",
    "        temp_reverse = [x.strip() for x in  temp['punc_truth'].to_list()  if x.strip()!='']\n",
    "        temp_rumor_class = len(temp_rumor)*[1]\n",
    "        temp_reverse_class = len(temp_reverse)*[0]\n",
    "        temp_data = temp_rumor + temp_reverse\n",
    "        temp_data_class = temp_rumor_class + temp_reverse_class\n",
    "        for i in range( len(temp_data)):\n",
    "            temp_data[i]=re.sub(r'[\\,\\'\\ ]|(\\n)','',temp_data[i])\n",
    "    data = data + temp_data\n",
    "    data_class = data_class + temp_data_class\n",
    "with open('data.txt','w') as f:\n",
    "    for i in range(len(data)):\n",
    "        f.write(data[i]+'\\t'+str(data_class[i])+'\\n')\n",
    "        data_list =list( zip(data, data_class))\n",
    "random.shuffle(data_list)\n",
    "data, data_class = zip(*data_list)\n",
    "index = ['rumor{}{}'.format(i, '!' * j) for (i, j) in zip(range(len(data)), data_class)]\n",
    "df = pd .DataFrame(data=zip(data_class,data), columns=[\"rumor\",\"text\"], index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/f0/6j6j3w814n53skbt2s4d8p5m0000gn/T/jieba.cache\n",
      "Loading model cost 0.358 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# jieba分词\n",
    "import jieba\n",
    "# 去除停用词\n",
    "with open ('hit_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    " \n",
    "seg_list = []\n",
    "for i in range(len(df)):\n",
    "    words = jieba.cut(df[\"text\"].iloc[i],cut_all=False)\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    seg_list.append( ' '.join(words))\n",
    " \n",
    "df['seg_text'] = seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenyu/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize,ngram_range=(1,2))\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=df.seg_text).toarray()#这句话计算了tfidf\n",
    "words = tfidf_model.get_feature_names_out()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarities = cosine_similarity(tfidf_docs)\n",
    "cos_avg = sum([sum(row) for row in cosine_similarities])/len(cosine_similarities)\n",
    "cos_sim = [sum(row)/cos_avg for row in cosine_similarities]\n",
    "df['cos_sim'] = cos_sim\n",
    "def get_word_vector(word):\n",
    "    return ft.get_word_vector(word)\n",
    "for i,word in enumerate(words):\n",
    "    if i == 0:\n",
    "        word_vectors = get_word_vector(word)\n",
    "    else:\n",
    "        word_vectors = np.vstack((word_vectors,get_word_vector(word)))\n",
    "word_dict = dict(zip(words,word_vectors))\n",
    "np.savez('../dataset/word_dict.npz',word_dict = word_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snowNLP情感分析\n",
    "from snownlp import SnowNLP\n",
    "sentimentslist = []\n",
    "for i in (df['text']):\n",
    "    a1 = SnowNLP(i)\n",
    "    a2 = a1.sentiments\n",
    "    sentimentslist.append(a2)\n",
    "avg_len = sum([len(x) for x in df['text']])/len(df['text'])\n",
    "df['length'] = [len(x)/avg_len for x in df['text']]\n",
    "df['sentiments'] = sentimentslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rumor0!</th>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor1!</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor2</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor3</th>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor4!</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor5</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "rumor0!   0.042  -0.031   0.034   0.107   0.134   0.056  -0.100   0.165   \n",
       "rumor1!  -0.003  -0.004  -0.011  -0.044  -0.001  -0.009  -0.030   0.000   \n",
       "rumor2   -0.007  -0.021  -0.019  -0.058  -0.003   0.018  -0.040  -0.013   \n",
       "rumor3    0.028  -0.028  -0.051   0.060   0.027  -0.027  -0.004  -0.039   \n",
       "rumor4!   0.006  -0.018  -0.016   0.015  -0.008   0.090   0.027   0.031   \n",
       "rumor5    0.006  -0.011  -0.026  -0.016   0.007   0.018  -0.008  -0.020   \n",
       "\n",
       "         topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "rumor0!   0.114  -0.088  ...    0.078    0.015   -0.022    0.012   -0.005   \n",
       "rumor1!  -0.044   0.030  ...    0.015    0.026   -0.007   -0.002   -0.004   \n",
       "rumor2   -0.054   0.006  ...    0.007    0.047   -0.002    0.017   -0.007   \n",
       "rumor3    0.022   0.005  ...   -0.023   -0.008   -0.030    0.010   -0.023   \n",
       "rumor4!   0.019   0.002  ...    0.000   -0.018   -0.004    0.011    0.018   \n",
       "rumor5    0.008   0.012  ...   -0.001   -0.010    0.019    0.027    0.006   \n",
       "\n",
       "         topic95  topic96  topic97  topic98  topic99  \n",
       "rumor0!    0.025    0.008   -0.005   -0.029   -0.033  \n",
       "rumor1!   -0.007   -0.003    0.006    0.006    0.010  \n",
       "rumor2    -0.010   -0.027   -0.012   -0.031   -0.020  \n",
       "rumor3     0.022   -0.057   -0.004   -0.018   -0.050  \n",
       "rumor4!   -0.015   -0.010   -0.015    0.021   -0.002  \n",
       "rumor5    -0.007   -0.010    0.019    0.006   -0.033  \n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "k=min(len(tfidf_model.vocabulary_),len(df))\n",
    "k=100\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "base = sum(pca.singular_values_)\n",
    "nums = pca.singular_values_.tolist()\n",
    "pca = PCA(n_components=k)\n",
    "pca_docs = pca.fit_transform(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca_docs.shape[1])]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns, index=index)\n",
    "pca_topic_vectors.round(3).head(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca_topic_vectors'] = pca_topic_vectors.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stevenyu/opt/anaconda3/envs/d2l/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic90</th>\n",
       "      <th>topic91</th>\n",
       "      <th>topic92</th>\n",
       "      <th>topic93</th>\n",
       "      <th>topic94</th>\n",
       "      <th>topic95</th>\n",
       "      <th>topic96</th>\n",
       "      <th>topic97</th>\n",
       "      <th>topic98</th>\n",
       "      <th>topic99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rumor0!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor1!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rumor4!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "rumor0!     0.0     0.0    0.00     0.0     0.0     0.0     0.0     0.0   \n",
       "rumor1!     0.0     0.0    0.80     0.0     0.0     0.0     0.0     0.0   \n",
       "rumor2      0.0     0.0    0.75     0.0     0.0     0.0     0.0     0.0   \n",
       "rumor3      0.0     0.0    0.00     0.0     0.8     0.0     0.0     0.0   \n",
       "rumor4!     0.0     0.0    0.00     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "         topic8  topic9  ...  topic90  topic91  topic92  topic93  topic94  \\\n",
       "rumor0!     0.0     0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "rumor1!     0.0     0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "rumor2      0.0     0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "rumor3      0.0     0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "rumor4!     0.0     0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "         topic95  topic96  topic97  topic98  topic99  \n",
       "rumor0!      0.0      0.0      0.0      0.0      0.0  \n",
       "rumor1!      0.0      0.0      0.0      0.0      0.0  \n",
       "rumor2       0.0      0.0      0.0      0.0      0.0  \n",
       "rumor3       0.0      0.0      0.0      0.0      0.0  \n",
       "rumor4!      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=df.text)\n",
    "                        .toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\n",
    "                                     counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia = LDiA(n_components=k, learning_method='batch')\n",
    "ldia = ldia.fit(bow_docs)  # <1>\n",
    "ldia.components_.shape\n",
    "ldia32_topic_vectors = ldia.transform(bow_docs)\n",
    "columns32 = ['topic{}'.format(i) for i in range(ldia.components_.shape[0])]\n",
    "ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors,index=index, columns=columns32)\n",
    "ldia32_topic_vectors.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ldia_topic_vectors'] = ldia32_topic_vectors.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../dataset/data.csv',encoding='utf-8')\n",
    "data = pd.read_csv('../dataset/data.csv',encoding='utf-8',index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpiaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
